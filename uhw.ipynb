{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf         \n",
    "from keras.layers import BatchNormalization, LeakyReLU, Add\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import Dense, Activation, Reshape\n",
    "from keras.models import Input, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(\"./collected_data\"):\n",
    "    os.makedirs(\"./collected_data\")\n",
    "    \n",
    "if not os.path.isdir(\"./collected_models\"):\n",
    "    os.makedirs(\"./collected_models\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing import image as IM\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_image = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_images_from_files_to_list(root, files, list, next_index, data_cb=None):\n",
    "    if files and next_index < num_image:\n",
    "        def identity(v):\n",
    "            return v\n",
    "\n",
    "        if data_cb is None:\n",
    "            data_cb = identity\n",
    "\n",
    "        for index, file in enumerate(files):\n",
    "            if file.endswith('.png'):\n",
    "                image = IM.load_img(os.path.join(root, file))\n",
    "                if image is not None and next_index < num_image:\n",
    "                    list[next_index] = data_cb(image.reshape((64, 64, 3)))\n",
    "                    next_index += 1\n",
    "                # list.append(data_cb(image[:, :, 0]))\n",
    "    return next_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_train = np.empty((num_image, 64, 64, 3))\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for root, sub_directories, files in os.walk(\"\"):\n",
    "    if i < num_image:\n",
    "        print(i)\n",
    "        i = add_images_from_files_to_list(root, files, images_train, i)\n",
    "\n",
    "# plt.subplot(96, 96)\n",
    "# plt.imshow(images_train[20], interpolation='nearest', cmap='gray_r')\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('./collected_data/gan_generated_image_epoch_%d.png' % 999)\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# images_train = np.array(images_train)\n",
    "# images_train = images_train.reshape((*images_train.shape, 1))\n",
    "# images_train = images_train[:, :, :, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout = 0.4\n",
    "depth = 256\n",
    "dim = 8\n",
    "input_dim = 128\n",
    "batch_size = 1\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Subpixel(Conv2D):\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 r,\n",
    "                 padding='same',\n",
    "                 data_format=None,\n",
    "                 strides=(1,1),\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(Subpixel, self).__init__(\n",
    "            filters=r*r*filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs)\n",
    "        self.r = r\n",
    "\n",
    "    def _phase_shift(self, I):\n",
    "        r = self.r\n",
    "        bsize, a, b, c = I.get_shape().as_list()\n",
    "        bsize = K.shape(I)[0] # Handling Dimension(None) type for undefined batch dim\n",
    "        X = K.reshape(I, [bsize, a, b, int(c/(r*r)),r, r]) # bsize, a, b, c/(r*r), r, r\n",
    "        X = K.permute_dimensions(X, (0, 1, 2, 5, 4, 3))  # bsize, a, b, r, r, c/(r*r)\n",
    "        #Keras backend does not support tf.split, so in future versions this could be nicer\n",
    "        X = [X[:,i,:,:,:,:] for i in range(a)] # a, [bsize, b, r, r, c/(r*r)\n",
    "        X = K.concatenate(X, 2)  # bsize, b, a*r, r, c/(r*r)\n",
    "        X = [X[:,i,:,:,:] for i in range(b)] # b, [bsize, r, r, c/(r*r)\n",
    "        X = K.concatenate(X, 2)  # bsize, a*r, b*r, c/(r*r)\n",
    "        return X\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self._phase_shift(super(Subpixel, self).call(inputs))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        unshifted = super(Subpixel, self).compute_output_shape(input_shape)\n",
    "        return (unshifted[0], int(self.r*unshifted[1]), int(self.r*unshifted[2]), int(unshifted[3]/(self.r*self.r)))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Conv2D, self).get_config()\n",
    "        config.pop('rank')\n",
    "        config.pop('dilation_rate')\n",
    "        config['filters']/=int(self.r*self.r)\n",
    "        config['r'] = self.r\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AnimeGeneratorFactory():\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "            Returns a generator Model described here: https://arxiv.org/pdf/1708.05509.pdf\n",
    "            \n",
    "            Args:\n",
    "                input_same: A 3 length tuple describing (width, height, channel)\n",
    "                \n",
    "            Output:\n",
    "                Keras Model\n",
    "        \"\"\"\n",
    "        MOMENTUM = 0.9\n",
    "        DIM = 16\n",
    "        DEPTH = 64\n",
    "        NUM_RESIDUAL = 16\n",
    "        NUM_SUBPIXEL = 2\n",
    "        FINAL_FILTERS = 3\n",
    "        #FINAL_FILTERS = 1\n",
    "        INITIAL_FILTERS = 64\n",
    "        def residual_block(layer, filters, momentum):\n",
    "            \"\"\"\n",
    "                Residual Block consisting of\n",
    "                    Conv2D -> Batch Normalization -> relu -> Conv2D -> Batch Normalization -> Residual Addition\n",
    "                \n",
    "                Args:\n",
    "                    layer:   Keras Layer\n",
    "                    filters: output size as an integer\n",
    "                    momentum: variable for batch normalization\n",
    "                \n",
    "                Returns:\n",
    "                    Keras layer\n",
    "            \"\"\"\n",
    "            shortcut = layer\n",
    "            layer = Conv2DTranspose(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(layer)\n",
    "            layer = BatchNormalization(momentum=momentum)(layer)\n",
    "            layer = Activation('relu')(layer)\n",
    "            layer = Conv2DTranspose(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(layer)\n",
    "            layer = BatchNormalization(momentum=momentum)(layer)\n",
    "\n",
    "            layer = Add()([layer, shortcut])\n",
    "            return layer\n",
    "\n",
    "        def residual_layer(layer, number, filters, momentum):\n",
    "            \"\"\"\n",
    "                Facade for residual block.\n",
    "                \n",
    "                Creates Residual layer with specified number of residual blocks\n",
    "                \n",
    "                Args:\n",
    "                    layer:   Keras layer\n",
    "                    number:  number of residual blocks in layer\n",
    "                    filters: output size as an integer\n",
    "                    momentum: variable for batch normalization\n",
    "                \n",
    "                Returns:\n",
    "                    Keras layer\n",
    "            \"\"\"\n",
    "            for _ in range(number):\n",
    "                layer = residual_block(layer, filters, momentum)\n",
    "            return layer\n",
    "\n",
    "        def subpixel_block(layer, filters, momentum):\n",
    "            \"\"\"\n",
    "                sub-pixel block consisting of\n",
    "                    Conv2D -> pixel shuffler x 2 -> Batch Normalization -> Relu\n",
    "                    \n",
    "                the code of subpixel layer is based on https://github.com/Tetrachrome/subpixel\n",
    "                    \n",
    "                Args:\n",
    "                    layer:   Keras Layer\n",
    "                    filters: output size as an integer\n",
    "                    momentum: variable for batch normalization\n",
    "                \n",
    "                Returns:\n",
    "                    Keras layer\n",
    "            \"\"\"\n",
    "\n",
    "            layer = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(layer)\n",
    "            layer = Subpixel(filters, (3, 3), 2)(layer)\n",
    "\n",
    "            layer = BatchNormalization(momentum=momentum)(layer)\n",
    "            layer = Activation('relu')(layer)\n",
    "            return layer\n",
    "\n",
    "        def subpixel_layer(layer, number, filters, momentum):\n",
    "            \"\"\"\n",
    "                Facade for subpixel block.\n",
    "                \n",
    "                Creates subpixel layer with specified number of subpixel blocks\n",
    "                \n",
    "                Args:\n",
    "                    layer:   Keras layer\n",
    "                    number:  number of subpixel blocks in layer\n",
    "                    filters: output size as an integer\n",
    "                    momentum: variable for batch normalization\n",
    "                \n",
    "                Returns:\n",
    "                    Keras layer\n",
    "            \"\"\"\n",
    "            for _ in range(number):\n",
    "                layer = subpixel_block(layer, filters, momentum)\n",
    "            return layer\n",
    "        \n",
    "        \n",
    "        inputs = Input(shape=input_shape)\n",
    "        filters = INITIAL_FILTERS\n",
    "        layer = Dense(DEPTH * DIM * DIM)(inputs)\n",
    "\n",
    "        layer = BatchNormalization(momentum=MOMENTUM)(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "        layer = Reshape((DIM, DIM, DEPTH))(layer)\n",
    "        old = layer\n",
    "        \n",
    "        # 16 residual layers\n",
    "        layer = residual_layer(layer, NUM_RESIDUAL, filters, MOMENTUM)\n",
    "\n",
    "        layer = BatchNormalization(momentum=MOMENTUM)(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "        layer = Add()([layer, old])\n",
    "\n",
    "        filters *= 4\n",
    "        # 3 sub-pixel layers\n",
    "        layer = subpixel_layer(layer, NUM_SUBPIXEL, filters, MOMENTUM)\n",
    "\n",
    "        layer = Conv2D(filters=FINAL_FILTERS, kernel_size=(9, 9), strides=(1, 1), padding=\"same\")(layer)\n",
    "        outputs = Activation('tanh')(layer)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AnimeDiscriminatorFactory(object):\n",
    "    \"\"\"\n",
    "        Discriminator Factory Class that creates the model described here:\n",
    "        https://arxiv.org/pdf/1708.05509.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "            Returns a Model described here: https://arxiv.org/pdf/1708.05509.pdf\n",
    "            Args:\n",
    "                input_same: A 3 length tuple describing (width, height, channel)\n",
    "            Output:\n",
    "                Keras Model\n",
    "        \"\"\"\n",
    "        RESIDUAL_BLOCKS_PER_LAYER = 2\n",
    "        LEAKY_RELU_ALPHA = 0.2\n",
    "        MODULES = 5\n",
    "\n",
    "        def intermediate_layer(layer, filters, kernel_size):\n",
    "            \"\"\"\n",
    "                Create the intermediate layers between residual layers.\n",
    "                Args:\n",
    "                    layer:       Keras layer\n",
    "                    filters:     output size as an integer\n",
    "                    kernel_size: length 2 tuple\n",
    "                Returns:\n",
    "                    Keras layer\n",
    "            \"\"\"\n",
    "            layer = Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                           strides=(2, 2), padding=\"same\")(layer)\n",
    "            layer = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(layer)\n",
    "            return layer\n",
    "\n",
    "        def initial_layer(input_layer):\n",
    "            \"\"\"\n",
    "                Facade for intermediate_layer for the first layer of the network.\n",
    "                Args:\n",
    "                    input_layer: Keras Input Layer\n",
    "                Returns:\n",
    "                    Keras layer\n",
    "            \"\"\"\n",
    "            INITIAL_LAYER_FILTER = 32\n",
    "            INITIAL_KERNEL_SIZE = (4, 4)\n",
    "            return intermediate_layer(input_layer, INITIAL_LAYER_FILTER, INITIAL_KERNEL_SIZE)\n",
    "\n",
    "        def residual_block(layer, filters):\n",
    "            \"\"\"\n",
    "                Residual Block consisting of\n",
    "                    Conv2D -> LeakyReLU -> Conv2D -> LeakyReLU -> Residual Addition\n",
    "                Args:\n",
    "                    layer:   Keras Layer\n",
    "                    filters: output size as an integer\n",
    "                Returns:\n",
    "                    Keras layer\n",
    "            \"\"\"\n",
    "            shortcut = layer\n",
    "            layer = Conv2D(filters=filters, kernel_size=(3, 3),\n",
    "                           strides=(1, 1), padding=\"same\")(layer)\n",
    "            layer = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(layer)\n",
    "            layer = Conv2D(filters=filters, kernel_size=(3, 3),\n",
    "                           strides=(1, 1), padding=\"same\")(layer)\n",
    "\n",
    "            layer = Add()([layer, shortcut])\n",
    "            layer = LeakyReLU(alpha=LEAKY_RELU_ALPHA)(layer)\n",
    "            return layer\n",
    "\n",
    "        def residual_layer(layer, number, filters):\n",
    "            \"\"\"\n",
    "                Facade for residual block.\n",
    "                Creates Residual layer with specified number of residual blocks\n",
    "                Args:\n",
    "                    layer:   Keras layer\n",
    "                    number:  number of residual blocks in layer\n",
    "                    filters: output size as an integer\n",
    "                Returns:\n",
    "                    Keras layer\n",
    "            \"\"\"\n",
    "            for _ in range(number):\n",
    "                layer = residual_block(layer, filters)\n",
    "            return layer\n",
    "\n",
    "        # NOTE: notation kxnysz\n",
    "        # - k specifies that the convolution layer has kernel_size x\n",
    "        # - n specifies that the convolution layer has y feature maps\n",
    "        # - s specifies that the convolution layer has stride z\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "\n",
    "        filters = 32\n",
    "        # initial layer k4n32s2\n",
    "        layer = initial_layer(inputs)\n",
    "        for i in range(MODULES):\n",
    "            layer = residual_layer(layer, RESIDUAL_BLOCKS_PER_LAYER, filters)\n",
    "            filters *= 2\n",
    "\n",
    "            intermediate_kernel_size = (3, 3)\n",
    "            if i < 2:\n",
    "                intermediate_kernel_size = (4, 4)\n",
    "            layer = intermediate_layer(\n",
    "                layer, filters, intermediate_kernel_size)\n",
    "\n",
    "        outputs = Dense(1, activation=\"sigmoid\")(layer)\n",
    "        \n",
    "        reshaped_output = Reshape((1,))(outputs)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=reshaped_output)\n",
    "        model.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = '../dataset-download/faces/'\n",
    "valid_data_dir = '../dataset-download/faces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting layer built\n",
      "residual layer built\n",
      "sub-pixel layer built\n",
      "final layer built\n"
     ]
    }
   ],
   "source": [
    "# generator1 = keras_generator()\n",
    "# generator2 = keras_generator()\n",
    "# generator3 = keras_generator()\n",
    "\n",
    "generator = AnimeGeneratorFactory().build([input_dim])\n",
    "# generator.summary()\n",
    "\n",
    "ganInput = Input((input_dim,))\n",
    "\n",
    "# x1 = generator1(ganInput)\n",
    "# x2 = generator2(ganInput)\n",
    "# x3 = generator3(ganInput)\n",
    "\n",
    "x = generator(ganInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discriminator = AnimeDiscriminatorFactory().build((64, 64, 3))\n",
    "# discriminator.summary()\n",
    "# d_input = keras.layers.concatenate([x1, x2, x3], axis=-1)\n",
    "# d_input = Reshape((96, 96, 3))(d_input)\n",
    "# generator = Model(inputs=ganInput, outputs=d_input)\n",
    "# ganOutput = discriminator(d_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ganOutput = discriminator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gan = Model(inputs=ganInput, outputs=ganOutput)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dLosses = []\n",
    "gLosses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the loss from each batch\n",
    "def plotLoss(epoch):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(dLosses, label='Discriminitive loss')\n",
    "    # plt.plot(gLosses, label='Generative loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('./collected_data/gan_loss_epoch_%d.png' % epoch)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a wall of generated MNIST images\n",
    "def plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
    "    noise = np.random.normal(0, 1, size=[examples, input_dim])\n",
    "    generatedImages = generator.predict(noise)\n",
    "    generatedImages = generatedImages.reshape(examples, 64, 64, 3)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generatedImages.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generatedImages[i], interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./collected_data/gan_generated_image_epoch_%d.png' % epoch)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the generator and discriminator networks (and weights) for later use\n",
    "def saveModels(epoch):\n",
    "    print(\"saving\")\n",
    "    generator.save('./collected_models/gan_generator1_epoch_%d.h5' % epoch)\n",
    "    # generator1.save('./collected_models/gan_generator1_epoch_%d.h5' % epoch)\n",
    "    # generator2.save('./collected_models/gan_generator2_epoch_%d.h5' % epoch)\n",
    "    # generator3.save('./collected_models/gan_generator3_epoch_%d.h5' % epoch)\n",
    "    discriminator.save('./collected_models/gan_discriminator_epoch_%d.h5' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epochs=1, batchSize=128):\n",
    "    batchCount = int(images_train.shape[0] / batchSize)\n",
    "    print('Epochs:', epochs)\n",
    "    print('Batch size:', batchSize)\n",
    "    print('Batches per epoch:', batchCount)\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "    valid_datagen = ImageDataGenerator(rescale = (1./255))\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=batchSize,\n",
    "        class_mode='binary')\n",
    "    \n",
    "    valid_generator = valid_datagen.flow_from_directory(\n",
    "        valid_data_dir,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=batchSize,\n",
    "        class_mode='binary')\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        dloss = 0\n",
    "        gloss = 0\n",
    "        for batch, label in train_generator:\n",
    "            # Get a random set of input noise and images\n",
    "            noise = np.random.normal(0, 1, size=[batchSize, input_dim])\n",
    "            #imageBatch = images_train[np.random.randint(0, images_train.shape[0], size=batchSize)]\n",
    "\n",
    "            # Generate fake images\n",
    "            generatedImages = generator.predict(noise)\n",
    "            # print(np.shape(imageBatch), np.shape(generatedImages))\n",
    "            images = np.concatenate([batch, generatedImages])        \n",
    "            # print(X.shape)\n",
    "\n",
    "            # Labels for generated and real data\n",
    "            labels = np.concatenate((label, np.ones(batchSize)))\n",
    "            \n",
    "            print(images.shape)\n",
    "            print(labels.shape)\n",
    "            \n",
    "            # Train discriminator\n",
    "            print(\"discriminator start...\", end=\"\")\n",
    "            discriminator.trainable = True\n",
    "            (dloss, dAcc) = discriminator.train_on_batch(images, labels)\n",
    "            print((dloss, dAcc))\n",
    "            print(\"done\")\n",
    "\n",
    "            # Train generator\n",
    "#             print(\"generator start...\", end=\"\")\n",
    "#             noise = np.random.normal(0, 1, size=[batchSize, input_dim])\n",
    "#             yGen = np.ones(batchSize)\n",
    "#             discriminator.trainable = False\n",
    "#             gloss = gan.train_on_batch(noise, yGen)\n",
    "# #             print('-'*5, 'Batch %d/%d done' % ((b + 1), batchCount))\n",
    "#             print(\"done\")\n",
    "#             break\n",
    "        \n",
    "        # Store loss of most recent batch from this epoch\n",
    "        dLosses.append(dloss)\n",
    "        gLosses.append(gloss)\n",
    "        \n",
    "#         print(\"saving generated image...\", end=\"\")\n",
    "#         for i in range(generatedImages.shape[0]):\n",
    "#             g_image = array_to_img(generatedImages[i])\n",
    "#             g_image.save(\"./collected_data/gan_generated_image_epoch_{0}_{1}.png\".format(e, i))\n",
    "#         print(\"done\")\n",
    "        \n",
    "        \n",
    "#         if e == 1 or e % 20 == 0:\n",
    "#             print(\"saving model...\", end=\"\")\n",
    "#             saveModels(e)\n",
    "#             print(\"done\")\n",
    "\n",
    "        # Plot losses from every epoch\n",
    "#         print(\"plotting losses...\", end=\"\")\n",
    "#         plotLoss(e)\n",
    "#         print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1\n",
      "Batch size: 32\n",
      "Batches per epoch: 1562\n",
      "Found 521 images belonging to 1 classes.\n",
      "Found 521 images belonging to 1 classes.\n",
      "--------------- Epoch 1 ---------------\n",
      "(64, 64, 64, 3)\n",
      "(64,)\n",
      "discriminator start...(7.9711919, 0.5)\n",
      "done\n",
      "(64, 64, 64, 3)\n",
      "(64,)\n",
      "discriminator start...(7.9711919, 0.5)\n",
      "done\n",
      "(64, 64, 64, 3)\n",
      "(64,)\n",
      "discriminator start...(7.9711919, 0.5)\n",
      "done\n",
      "(64, 64, 64, 3)\n",
      "(64,)\n",
      "discriminator start...(7.9711919, 0.5)\n",
      "done\n",
      "(64, 64, 64, 3)\n",
      "(64,)\n",
      "discriminator start...(7.9711919, 0.5)\n",
      "done\n",
      "(64, 64, 64, 3)\n",
      "(64,)\n",
      "discriminator start...(7.9711919, 0.5)\n",
      "done\n",
      "(64, 64, 64, 3)\n",
      "(64,)\n",
      "discriminator start...(7.9711919, 0.5)\n",
      "done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-6f0050a177ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-15884d8478c6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batchSize)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Generate fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mgeneratedImages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;31m# print(np.shape(imageBatch), np.shape(generatedImages))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneratedImages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1713\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(1, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.metrics_names\n",
    "gan.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(batch_size):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    \n",
    "    return train_generator\n",
    "\n",
    "def build_network(input_shape, noise_shape):\n",
    "    discriminator = AnimeDiscriminatorFactory().build(input_shape)\n",
    "    generator = AnimeGeneratorFactory().build(noise_shape)\n",
    "    \n",
    "    gan_inputs = Input(noise_shape)\n",
    "    generator_outputs = generator(gan_inputs)\n",
    "    gan_outputs = discriminator(generator_outputs)\n",
    "    gan = Model(inputs=gan_inputs, outputs=gan_outputs)\n",
    "    gan.compile(loss = \"binary_crossentropy\",\n",
    "                optimizer = \"adam\",\n",
    "                metrics=[\"accuracy\"])\n",
    "    return (discriminator, generator, gan)\n",
    "    \n",
    "def execute(epochs, input_shape, noise_shape, train_generator, discriminator, generator, gan):\n",
    "    e = 1\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    for batch, label in train_generator:\n",
    "        dloss = 0\n",
    "        gloss = 0\n",
    "        \n",
    "        print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        \n",
    "        # Get a random set of input noise and images\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, ) + noise_shape)\n",
    "        generated_images = generator.predict(noise)\n",
    "        images = np.concatenate([batch, generated_images])\n",
    "\n",
    "        # Labels for generated and real data\n",
    "        discriminator_labels = np.concatenate((label, np.ones(batch_size)))\n",
    "\n",
    "        # Train discriminator\n",
    "        print(\"discriminator start...\", end=\"\")\n",
    "        discriminator.trainable = True\n",
    "        d_loss, d_acc = discriminator.train_on_batch(images, discriminator_labels)\n",
    "        print(\"done\")\n",
    "\n",
    "        # Train generator\n",
    "        print(\"generator start...\", end=\"\")\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, ) + noise_shape)\n",
    "        generator_labels = np.ones(batch_size)\n",
    "        discriminator.trainable = False\n",
    "        g_loss, g_acc = gan.train_on_batch(noise, generator_labels)\n",
    "        print(\"done\")\n",
    "\n",
    "        # Store loss of most recent batch from this epoch\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "        print(\"dloss: \", (d_loss, d_acc))\n",
    "        print(\"gloss: \", (g_loss, g_acc))\n",
    "\n",
    "        if e % 1000 == 0:\n",
    "            print(\"saving generated image...\", end=\"\")\n",
    "            for i in range(generated_images.shape[0]):\n",
    "                g_image = array_to_img(generated_images[i])\n",
    "                g_image.save(\"./collected_data/gan_generated_image_epoch_{0}_{1}.png\".format(e, i))\n",
    "            print(\"done\")\n",
    "\n",
    "        if e == 1 or e % 20 == 0:\n",
    "            print(\"saving model...\", end=\"\")\n",
    "            saveModels(e)\n",
    "            print(\"done\")\n",
    "\n",
    "        # Plot losses from every epoch\n",
    "        # print(\"plotting losses...\", end=\"\")\n",
    "        # plotLoss(e)\n",
    "        # print(\"done\")\n",
    "\n",
    "        if e >= epochs:\n",
    "            break\n",
    "        e += 1\n",
    "    \n",
    "def train(epochs, batch_size, input_shape, noise_shape):\n",
    "    train_generator = data_generator(batch_size)\n",
    "    discriminator, generator, gan = build_network(input_shape, noise_shape)\n",
    "    execute(epochs, input_shape, noise_shape,\n",
    "            train_generator, discriminator, generator, gan)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 521 images belonging to 1 classes.\n",
      "--------------- Epoch 1 ---------------\n",
      "discriminator start...done\n",
      "generator start...done\n",
      "dloss:  (2.3697796, 0.060606062)\n",
      "gloss:  (16.118095, 0.0)\n",
      "saving model...saving\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train(1, 32, (64, 64, 3), (1, 1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2,) + (1, 1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
